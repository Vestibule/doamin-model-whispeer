# Impl√©mentation LLM Router + MCP Integration

## Vue d'ensemble

Ce projet int√®gre un routeur LLM qui g√©n√®re des DomainModel JSON stricts √† partir de transcripts en langage naturel. Le LLM est contraint par un system prompt strict pour ne produire **que** du JSON conforme au sch√©ma DomainModel.

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CLI / Tauri    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MCP Server    ‚îÇ ‚Üê generate_domain_model tool
‚îÇ  (mcp-server)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   LLM Router    ‚îÇ ‚Üê Routes to Ollama or External
‚îÇ  (llm_router)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    v         v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ollama ‚îÇ ‚îÇ External ‚îÇ
‚îÇ (local)‚îÇ ‚îÇ Provider ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Composants

### 1. LLM Router (`src-tauri/src/llm_router.rs`)

**Responsabilit√©s :**
- D√©tecte le provider LLM depuis les variables d'environnement
- Route les requ√™tes vers Ollama (`POST /api/generate`) ou provider externe
- Parse les r√©ponses JSON en `DomainModelResponse`

**Configuration via `.env` :**
```bash
# Ollama
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Ou External Provider
LLM_PROVIDER=openai
LLM_API_KEY=sk-...
LLM_ENDPOINT=https://api.openai.com/v1/chat/completions
```

**API principale :**
```rust
pub async fn generate_domain_model(
    &self,
    system_prompt: &str,
    user_prompt: &str,
) -> Result<DomainModelResponse>
```

### 2. MCP Server (`mcp/mcp-server/src/main.rs`)

**Outil ajout√© :**
```json
{
  "name": "generate_domain_model",
  "description": "Generate a complete DomainModel from natural language using LLM",
  "inputSchema": {
    "type": "object",
    "properties": {
      "transcript": {"type": "string"},
      "input_lang": {"type": "string", "default": "fr"}
    },
    "required": ["transcript"]
  }
}
```

**Handler :** Actuellement un placeholder, pr√™t pour int√©gration compl√®te du LLM router.

### 3. CLI Tool (`mcp/mcp-server/src/cli.rs`)

**Commande :**
```bash
cargo run --bin mcp-cli -- [--dry-run-llm] --input samples/transcript.jsonl
```

**Fonctionnalit√©s :**
- Lit un fichier JSONL de transcript
- Appelle le LLM (ou simule en dry-run)
- Affiche le DomainModel JSON g√©n√©r√©

**Exemple de sortie :**
```
üìù Transcript loaded (8 lines)
ü§ñ Mode: DRY-RUN

‚è≥ Generating DomainModel from transcript...
‚úÖ DomainModel generated successfully!

{
  "entities": [...],
  "relations": [...],
  "invariants": [...]
}
```

### 4. System Prompt Strict

Le LLM re√ßoit ce prompt syst√®me :

```
Tu es un normalizer de Domain Model. Rends UNIQUEMENT un JSON valide 
DomainModel conforme au schema. Interdis les champs non list√©s.

Schema DomainModel (STRICT - aucun champ suppl√©mentaire autoris√©):
{
  "entities": [...],
  "relations": [...],
  "invariants": [...]
}

R√àGLES STRICTES:
1. AUCUN champ en dehors de ce schema
2. Tous les champs "obligatoire" DOIVENT √™tre pr√©sents
3. Les types enum DOIVENT correspondre exactement
4. Les patterns regex DOIVENT √™tre respect√©s
5. R√©ponds UNIQUEMENT avec ce JSON, pas de tool_calls
```

## Sch√©ma DomainModel

Voir `mcp/domain_model.schema.json` pour le sch√©ma complet.

**R√©sum√© des types principaux :**

### Entity
```json
{
  "id": "string",
  "name": "string",
  "description": "string (optional)",
  "attributes": [
    {
      "name": "string",
      "type": "string|number|integer|boolean|date|datetime|email|url|uuid|json|text",
      "required": boolean,
      "unique": boolean
    }
  ],
  "primaryKey": ["string"],
  "uniqueConstraints": [...]
}
```

### Relation
```json
{
  "id": "string",
  "name": "string",
  "from": {"entityId": "string"},
  "to": {"entityId": "string"},
  "cardinality": {
    "from": "0..1|1|0..n|1..n|*",
    "to": "0..1|1|0..n|1..n|*"
  }
}
```

### Invariant
```json
{
  "id": "string",
  "name": "string",
  "type": "uniqueness|referential_integrity|domain_constraint|cardinality|business_rule|temporal|aggregation",
  "expression": "string",
  "severity": "error|warning|info"
}
```

## Utilisation

### 1. Configuration

```bash
# Dans src-tauri/ ou mcp/mcp-server/
cp .env.example .env
# √âditer .env avec vos credentials
```

### 2. Test avec le CLI

```bash
cd mcp/mcp-server

# Mode dry-run (simulation)
cargo run --bin mcp-cli -- --dry-run-llm --input ../../samples/transcript.jsonl

# Mode live (LLM r√©el)
cargo run --bin mcp-cli -- --input ../../samples/transcript.jsonl
```

### 3. Int√©gration Tauri

```rust
use domain_model_note_taking_lib::llm_integration::LlmIntegration;

let integration = LlmIntegration::new()?;
let domain_model = integration
    .process_request("User entity has email and password")
    .await?;
```

## Format des fichiers

### Transcript JSONL (`samples/transcript.jsonl`)

```jsonl
{"speaker": "user", "text": "Je veux mod√©liser un syst√®me de biblioth√®que"}
{"speaker": "user", "text": "Un Livre a un titre, un ISBN unique"}
{"speaker": "user", "text": "Un Auteur a un nom et une biographie"}
```

## Tests

```bash
# Test du CLI en dry-run
cd mcp/mcp-server
cargo run --bin mcp-cli -- --dry-run-llm --input ../../samples/transcript.jsonl

# Build release
cargo build --release --bin mcp-cli

# Compilation Tauri
cd src-tauri
cargo check --lib
```

## S√©curit√©

**IMPORTANT :** Ne jamais committer les fichiers `.env` !

Les `.gitignore` ont √©t√© mis √† jour pour exclure :
```
.env
```

Utilisez `.env.example` comme template pour d'autres d√©veloppeurs.

## Prochaines √©tapes

1. **Impl√©menter l'int√©gration compl√®te** du LLM router dans le handler MCP `generate_domain_model`
2. **Ajouter la validation** du DomainModel g√©n√©r√© contre le JSON schema
3. **Cr√©er des commandes Tauri** pour exposer `generate_domain_model` √† l'UI Vue
4. **Ajouter des tests d'int√©gration** avec diff√©rents providers LLM
5. **Optimiser le system prompt** bas√© sur les r√©sultats r√©els

## Documentation

- **LLM Router :** `src-tauri/LLM_ROUTER.md`
- **CLI Tool :** `mcp/mcp-server/CLI.md`
- **Sch√©ma DomainModel :** `mcp/domain_model.schema.json`
- **WARP Rules :** `WARP.md`

## D√©pendances ajout√©es

**`src-tauri/Cargo.toml` :**
```toml
reqwest = { version = "0.11", features = ["json"] }
dotenvy = "0.15"
```

**`mcp/mcp-server/Cargo.toml` :**
```toml
clap = { version = "4", features = ["derive"] }
dotenvy = "0.15"
reqwest = { version = "0.11", features = ["json"] }
```

## Principe cl√©

> **Le LLM ne parle jamais √† l'UI. Il n'√©met que des structures pour les tools.**

Le system prompt contraint strictement le LLM √† ne produire que du JSON DomainModel valide, sans aucun texte libre ni champs suppl√©mentaires.
