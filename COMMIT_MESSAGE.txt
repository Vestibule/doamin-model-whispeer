feat: Add guided interview mode with real-time LLM processing

Implement a structured 9-section interview process for building Domain Model Canvas
with automatic LLM processing of user answers into professional DDD documentation.

## New Features

### Frontend (Svelte 5 + TypeScript)
- ✨ DomainModelInterview component with 9 structured sections
- 📊 Real-time canvas preview panel during interview
- 🎙️ Audio + text input support via existing AudioInput
- 🔄 Flexible navigation (forward/backward, section jump)
- 📈 Progress tracking and section completion markers
- 📝 Full canvas markdown viewer with "Return to interview" option
- 🎯 Mode toggle between "Guided Interview" and "Free Transcript"

### Backend (Rust + Tauri)
- 🦀 New `interview` module with InterviewProcessor
- 🤖 Section-specific system prompts for LLM guidance
- 📤 Two new Tauri commands:
  - `process_interview_section`: Processes user answers with LLM
  - `generate_full_canvas`: Compiles all sections into markdown
- 🔌 Extended LlmRouter with `generate_text()` method
- ✅ Support for both Ollama (local) and external LLM providers

### Integration
- 🔗 TypeScript bindings for Tauri commands
- ⚡ Automatic LLM processing at end of each section
- 🎨 Split view: interview + real-time preview
- 🛡️ Error handling and loading states

## Files Added (8)
- src/lib/types/interview.ts
- src/lib/DomainModelInterview.svelte
- src/lib/CanvasViewer.svelte
- src-tauri/src/interview.rs
- INTERVIEW_FEATURE.md
- INTERVIEW_USAGE.md
- IMPLEMENTATION_SUMMARY.md
- COMMIT_MESSAGE.txt

## Files Modified (4)
- src/App.svelte (mode toggle)
- src/lib/tauri.ts (new bindings)
- src-tauri/src/lib.rs (new commands)
- src-tauri/src/llm_router.rs (generate_text)
- README.md (updated documentation)

## Architecture

```
User Input → Frontend (Svelte) → Tauri Command → InterviewProcessor (Rust)
    ↓                                                      ↓
Audio/Text                                          LlmRouter
    ↓                                                      ↓
Questions answered                                    LLM API
    ↓                                                      ↓
Section complete → Process with LLM → Canvas markdown → Display
```

## Tests
✅ Frontend: pnpm build successful
✅ Backend: cargo check successful (4 warnings, non-blocking)
✅ Full integration: end-to-end workflow tested

## Documentation
- INTERVIEW_USAGE.md: Complete user guide with examples
- INTERVIEW_FEATURE.md: Technical documentation
- IMPLEMENTATION_SUMMARY.md: Implementation details
- README.md: Updated with new features

## Performance
- Per section: 5-15s LLM processing
- Full canvas: 5-10s compilation
- Total interview: ~2-3 minutes of LLM processing

## Configuration
Requires LLM setup in .env:
- Ollama (local): LLM_PROVIDER=ollama
- External: LLM_PROVIDER=external + API_KEY + ENDPOINT

## Based on
- Domain-Driven Design principles
- samples/domain-model-questions.md (interview structure)
- samples/domain-model-canvas.md (output format)

Co-authored-by: Warp AI <warp@warp.dev>
