# MCP CLI Tool

CLI pour tester l'outil MCP `generate_domain_model` avec int√©gration LLM.

## Installation

```bash
cd mcp/mcp-server
cargo build --release --bin mcp-cli
```

## Configuration

Cr√©ez un fichier `.env` dans `mcp/mcp-server/` :

```bash
cp .env.example .env
# √âditez .env avec vos credentials
```

## Utilisation

### Mode Dry-Run (sans appel LLM r√©el)

Pour tester rapidement avec une r√©ponse simul√©e :

```bash
cargo run --bin mcp-cli -- --dry-run-llm --input ../../samples/transcript.jsonl
```

Ou avec le binaire compil√© :

```bash
./target/release/mcp-cli --dry-run-llm --input ../../samples/transcript.jsonl
```

### Mode Live (avec LLM r√©el)

Assurez-vous que votre `.env` est configur√©, puis :

```bash
cargo run --bin mcp-cli -- --input ../../samples/transcript.jsonl
```

## Format du fichier d'entr√©e

Le fichier d'entr√©e doit √™tre au format JSONL (JSON Lines) :

```jsonl
{"speaker": "user", "text": "Je veux mod√©liser un syst√®me de biblioth√®que"}
{"speaker": "user", "text": "Un Livre a un titre, un ISBN unique, et une date de publication"}
{"speaker": "user", "text": "Un Auteur a un nom et une biographie optionnelle"}
```

Chaque ligne est un objet JSON avec :
- `speaker` : identifiant du locuteur (peut √™tre ignor√©)
- `text` : le texte de la ligne de transcript

## Sortie

Le CLI g√©n√®re un DomainModel JSON conforme au sch√©ma avec :

- **entities** : liste des entit√©s du domaine
- **relations** : liste des relations entre entit√©s
- **invariants** : liste des invariants m√©tier

Exemple :

```json
{
  "entities": [
    {
      "id": "Livre",
      "name": "Livre",
      "attributes": [
        {"name": "titre", "type": "string", "required": true},
        {"name": "isbn", "type": "string", "required": true, "unique": true}
      ]
    }
  ],
  "relations": [...],
  "invariants": [...]
}
```

## Providers LLM support√©s

### Ollama (Local)

```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```

### OpenAI

```bash
LLM_PROVIDER=openai
LLM_API_KEY=sk-...
LLM_ENDPOINT=https://api.openai.com/v1/chat/completions
```

### Anthropic

```bash
LLM_PROVIDER=anthropic
LLM_API_KEY=sk-ant-...
LLM_ENDPOINT=https://api.anthropic.com/v1/messages
```

## Exemples

### Exemple 1 : Mode dry-run

```bash
$ cargo run --bin mcp-cli -- --dry-run-llm --input ../../samples/transcript.jsonl

üìù Transcript loaded (8 lines)
ü§ñ Mode: DRY-RUN

‚è≥ Generating DomainModel from transcript...
‚úÖ DomainModel generated successfully!

{
  "entities": [...],
  "relations": [...],
  "invariants": [...]
}
```

### Exemple 2 : Avec Ollama local

```bash
# S'assurer qu'Ollama tourne
$ ollama serve

# Dans un autre terminal
$ cargo run --bin mcp-cli -- --input ../../samples/transcript.jsonl

üìù Transcript loaded (8 lines)
ü§ñ Mode: LIVE LLM

‚è≥ Generating DomainModel from transcript...
‚úÖ DomainModel generated successfully!
...
```

## Int√©gration MCP

L'outil `generate_domain_model` est √©galement disponible comme outil MCP dans le serveur.

D√©finition de l'outil :

```json
{
  "name": "generate_domain_model",
  "description": "Generate a complete DomainModel from natural language using LLM",
  "inputSchema": {
    "type": "object",
    "properties": {
      "transcript": {
        "type": "string",
        "description": "Natural language transcript describing the domain model"
      },
      "input_lang": {
        "type": "string",
        "description": "Input language code (e.g., 'en', 'fr')",
        "default": "fr"
      }
    },
    "required": ["transcript"]
  }
}
```

## Contraintes du LLM

Le LLM est contraint par un system prompt strict qui impose :

1. **Aucun champ** en dehors du sch√©ma DomainModel
2. **Tous les champs obligatoires** doivent √™tre pr√©sents
3. **Les types enum** doivent correspondre exactement
4. **Les patterns regex** doivent √™tre respect√©s
5. **R√©ponse JSON uniquement**, pas de texte libre

Cela garantit que la sortie est toujours un DomainModel valide.
