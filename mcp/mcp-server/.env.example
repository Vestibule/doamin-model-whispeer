# LLM Provider Configuration for MCP Server

# ===== Option 1: Ollama (Local) =====
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# ===== Option 2: External Provider (e.g., OpenAI) =====
# LLM_PROVIDER=openai
# LLM_API_KEY=your_api_key_here
# LLM_ENDPOINT=https://api.openai.com/v1/chat/completions

# ===== Option 3: External Provider (e.g., Anthropic) =====
# LLM_PROVIDER=anthropic
# LLM_API_KEY=your_api_key_here
# LLM_ENDPOINT=https://api.anthropic.com/v1/messages
