# Backend Audio Recording avec Whisper

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend (Svelte)            â”‚
â”‚   - SpeechInput component      â”‚
â”‚   - Start/Stop buttons         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ invoke() commands
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RecordingManager (Rust)      â”‚
â”‚   - State management           â”‚
â”‚   - Event emission             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
      â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio   â”‚  â”‚  Whisper     â”‚
â”‚  Session â”‚  â”‚  STT         â”‚
â”‚  (VAD)   â”‚  â”‚  (stub)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Flux de donnÃ©es

### 1. DÃ©marrage de l'enregistrement

```
User clicks ğŸ¤
  â†’ startRecording()
  â†’ RecordingManager.start_recording()
  â†’ AudioSession.start_recording()
  â†’ VAD loop starts
  â†’ Emit "recording-state-changed": "recording"
```

### 2. Pendant l'enregistrement

```
Audio stream
  â†’ webrtc-vad detects voice activity
  â†’ Segments saved as WAV files
  â†’ Files: /tmp/domain-model-audio/utterance_XXXX.wav
```

### 3. ArrÃªt et transcription

```
User clicks ğŸ”´
  â†’ stopRecording()
  â†’ Recording stops
  â†’ Emit "recording-state-changed": "processing"
  â†’ For each utterance:
      â†’ SpeechToText.transcribe_file()
      â†’ Emit "transcription-result": { text, language, duration_ms }
  â†’ Emit "recording-state-changed": "idle"
```

## Configuration

### Variables d'environnement

```bash
# .env (optionnel)
WHISPER_MODEL_PATH=models/ggml-base.en.bin
```

### TÃ©lÃ©charger un modÃ¨le Whisper

```bash
mkdir -p models
cd models

# ModÃ¨le tiny (75 MB) - rapide
curl -LO https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.en.bin

# ModÃ¨le base (140 MB) - recommandÃ©
curl -LO https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin

# ModÃ¨le small (466 MB) - prÃ©cis
curl -LO https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small.en.bin
```

## ParamÃ¨tres VAD

Dans `audio_session.rs`, les paramÃ¨tres par dÃ©faut sont :

```rust
AudioSessionConfig {
    silence_duration_ms: 1000,        // 1s de silence pour terminer une utterance
    min_utterance_duration_ms: 300,   // Utterance minimale de 300ms
    vad_mode: VadMode::Aggressive,    // DÃ©tection agressive
    output_dir: /tmp/domain-model-audio
}
```

### Ajuster la sensibilitÃ©

Pour ajuster la dÃ©tection de parole, modifiez dans `lib.rs` :

```rust
let config = AudioSessionConfig {
    silence_duration_ms: 1500,  // Plus de tolÃ©rance au silence
    min_utterance_duration_ms: 500,  // Utterances plus longues
    vad_mode: VadMode::Quality,  // Moins agressif
    output_dir: output_dir.clone(),
};
```

## Events Tauri

Le backend Ã©met plusieurs Ã©vÃ©nements vers le frontend :

### `recording-state-changed`
```typescript
listen<string>('recording-state-changed', (event) => {
  // event.payload: "idle" | "recording" | "processing"
});
```

### `transcription-result`
```typescript
listen<TranscriptionResult>('transcription-result', (event) => {
  const { text, language, duration_ms } = event.payload;
});
```

### `transcription-error`
```typescript
listen<string>('transcription-error', (event) => {
  console.error('Transcription failed:', event.payload);
});
```

### `recording-error`
```typescript
listen<string>('recording-error', (event) => {
  console.error('Recording failed:', event.payload);
});
```

## Commandes Tauri

### `start_recording`
```typescript
import { startRecording } from './lib/tauri';

const status = await startRecording();
// Returns: "Recording started. Audio will be saved to: /tmp/domain-model-audio"
```

### `stop_recording`
```typescript
import { stopRecording } from './lib/tauri';

const status = await stopRecording();
// Returns: "Recording stopped. Processing utterances..."
```

### `transcribe_audio`
```typescript
import { transcribeAudio } from './lib/tauri';

const result = await transcribeAudio('/path/to/audio.wav');
// Returns: { text: string, language: string | null, duration_ms: number }
```

## Ã‰tat actuel de Whisper

### âš ï¸ Status: Stub Implementation

La transcription Whisper est actuellement en mode stub. Les fichiers audio sont enregistrÃ©s correctement, mais la transcription retourne :

```
[Transcription stub: audio file XXXX bytes - configure WHISPER_MODEL_PATH to enable real transcription]
```

### Pour activer Whisper

1. **TÃ©lÃ©charger un modÃ¨le** (voir section ci-dessus)

2. **Fixer l'API whisper-rs** dans `speech_to_text.rs`:
   - Remplacer le stub par l'implÃ©mentation rÃ©elle
   - Utiliser les mÃ©thodes correctes de l'API whisper-rs 0.15.1

3. **Tester**:
   ```bash
   WHISPER_MODEL_PATH=models/ggml-base.en.bin pnpm tauri dev
   ```

## Debugging

### VÃ©rifier les fichiers audio

```bash
ls -lh /tmp/domain-model-audio/
```

Les fichiers WAV devraient Ãªtre :
- Format: 16-bit PCM mono
- Sample rate: 16000 Hz
- NommÃ©s: `utterance_0001.wav`, `utterance_0002.wav`, etc.

### Logs backend

Les logs Rust apparaissent dans la console Tauri :

```
[INFO] Loading Whisper model from models/ggml-base.en.bin
[INFO] Starting audio recording thread
[INFO] Found 3 utterances to transcribe
[INFO] Transcribing utterance 1: /tmp/.../utterance_0001.wav
[WARN] Transcribing /tmp/.../utterance_0001.wav (stub - Whisper not yet configured)
```

### Tester l'audio manuellement

```bash
# Installer ffplay (part of ffmpeg)
brew install ffmpeg

# Ã‰couter un fichier enregistrÃ©
ffplay /tmp/domain-model-audio/utterance_0001.wav
```

## Performance

### Latence attendue

- **DÃ©tection VAD**: < 30ms
- **Sauvegarde WAV**: < 10ms
- **Transcription Whisper** (avec modÃ¨le):
  - tiny: ~100-200ms par utterance
  - base: ~200-500ms par utterance
  - small: ~500ms-1s par utterance

### Optimisations futures

1. **Transcription en streaming** : Transcrire pendant l'enregistrement
2. **ModÃ¨le en cache** : PrÃ©charger le modÃ¨le Whisper au dÃ©marrage
3. **GPU acceleration** : Utiliser CUDA/Metal si disponible
4. **Quantification** : Utiliser des modÃ¨les quantifiÃ©s (Q5, Q8)

## Limitations actuelles

1. **Pas de pause/resume** : L'enregistrement doit Ãªtre arrÃªtÃ© puis redÃ©marrÃ©
2. **Pas de feedback audio** : Pas d'indicateur du niveau sonore
3. **Pas de sÃ©lection de langue** : HardcodÃ© Ã  "en" (anglais)
4. **Whisper en stub** : NÃ©cessite configuration du modÃ¨le

## Roadmap

- [ ] ImplÃ©menter vraie transcription Whisper
- [ ] Ajouter feedback niveau audio visuel
- [ ] Support multilingue (auto-dÃ©tection)
- [ ] Transcription en streaming
- [ ] Export des utterances en JSON
- [ ] IntÃ©gration directe avec `orchestrate()`
